"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –æ–±—ã—á–Ω–æ–π –∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ä–µ–¥
–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –æ–±—É—á–µ–Ω–∏—è
"""

import numpy as np
import torch
import time
import os
import sys
from typing import Dict, List, Tuple

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from gridworld_env import GridWorldEnv
from task4.vectorized_wrapper import make_vectorized_env
from task4.vectorized_gridworld import VectorizedGridWorldEnv
from dqn import DQNAgent
from train import train_dqn
from task4.train_vectorized_simple import train_dqn_vectorized, evaluate_vectorized

try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False


def create_env_basic(env_id: int, seed: int = 42):
    """–°–æ–∑–¥–∞–µ—Ç –æ–±—ã—á–Ω—É—é (–Ω–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—É—é) —Å—Ä–µ–¥—É."""
    from task2.train_task1 import create_task1_env
    env, num_colors = create_task1_env(env_id, seed=seed)
    return env, num_colors


def run_comparison(
    env_id: int = 1,
    num_envs: int = 4,
    n_episodes: int = 500,
    max_steps: int = 200,
    save_dir: str = "task4/results",
    use_wandb: bool = False,
    wandb_project: str = "RL4",
):
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ–±—ã—á–Ω–æ–π, –Ω–∞–∏–≤–Ω–æ–π –∏ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–π –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ä–µ–¥.
    
    Args:
        env_id: ID –æ–∫—Ä—É–∂–µ–Ω–∏—è (1, 2, –∏–ª–∏ 3)
        num_envs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö —Å—Ä–µ–¥
        n_episodes: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (—Å—É–º–º–∞—Ä–Ω–æ)
        max_steps: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –≤ —ç–ø–∏–∑–æ–¥–µ
        save_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        use_wandb: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ Wandb
        wandb_project: –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞ –≤ Wandb
    """
    print("="*60)
    print("–°–†–ê–í–ù–ï–ù–ò–ï –û–ë–´–ß–ù–û–ô –ò –í–ï–ö–¢–û–†–ò–ó–û–í–ê–ù–ù–´–• –°–†–ï–î")
    print("="*60)
    print(f"–û–∫—Ä—É–∂–µ–Ω–∏–µ: {env_id}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö —Å—Ä–µ–¥: {num_envs}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤: {n_episodes}")
    print()
    
    results = {}
    
    # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    base_env, num_colors = create_env_basic(env_id, seed=42)
    
    # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–∫—Ä—É–∂–µ–Ω–∏—è
    grid_size = base_env.grid_size
    start_pos = base_env.start_pos
    goal_pos = base_env.goal_pos
    obstacles = base_env.obstacles
    floor_colors = base_env.floor_colors
    
    print(f"üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–∫—Ä—É–∂–µ–Ω–∏—è:")
    print(f"   –†–∞–∑–º–µ—Ä —Å–µ—Ç–∫–∏: {grid_size}x{grid_size}")
    print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ü–≤–µ—Ç–æ–≤: {num_colors}")
    print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–ø—è—Ç—Å—Ç–≤–∏–π: {len(obstacles)}")
    print()
    
    # 1. –û–±—ã—á–Ω–∞—è —Å—Ä–µ–¥–∞ (–Ω–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–∞—è)
    print("="*60)
    print("1. –û–ë–´–ß–ù–ê–Ø –°–†–ï–î–ê (–Ω–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–∞—è)")
    print("="*60)
    
    env_normal = GridWorldEnv(
        grid_size=grid_size,
        start_pos=start_pos,
        goal_pos=goal_pos,
        obstacles=obstacles,
        floor_colors=floor_colors,
        seed=42,
        max_steps=max_steps,
    )
    
    agent_normal = DQNAgent(
        obs_dim=1,
        action_dim=4,
        lr=1e-3,
        gamma=0.99,
        epsilon_start=1.0,
        epsilon_end=0.01,
        epsilon_decay=0.995,
        buffer_size=10000,
        batch_size=32,
        device='cuda' if torch.cuda.is_available() else 'cpu',
    )
    
    start_time = time.time()
    
    metrics_normal = train_dqn(
        env_normal,
        agent_normal,
        n_episodes=n_episodes,
        max_steps_per_episode=max_steps,
        train_freq=4,
        eval_freq=50,
        save_dir=None,
        use_wandb=use_wandb,
        wandb_config={
            'project': wandb_project,
            'name': f'normal-env{env_id}',
            'config': {
                'env_id': env_id,
                'num_envs': 1,
                'vectorized': False,
            }
        } if use_wandb else None,
    )
    
    time_normal = time.time() - start_time
    # –î–ª—è –æ–±—ã—á–Ω–æ–π —Å—Ä–µ–¥—ã total_steps = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤ * —Å—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —ç–ø–∏–∑–æ–¥–∞
    total_steps_normal = sum(metrics_normal['episode_lengths']) if metrics_normal['episode_lengths'] else 0
    
    results['normal'] = {
        'metrics': metrics_normal,
        'time': time_normal,
        'total_steps': total_steps_normal,
        'steps_per_second': total_steps_normal / time_normal if time_normal > 0 else 0,
        'final_reward': np.mean(metrics_normal['episode_rewards'][-100:]) if len(metrics_normal['episode_rewards']) >= 100 else 0,
        'num_episodes': len(metrics_normal['episode_rewards']),
    }
    
    print(f"‚úÖ –û–±—ã—á–Ω–∞—è —Å—Ä–µ–¥–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    print(f"   –í—Ä–µ–º—è: {time_normal:.2f}s")
    print(f"   –í—Å–µ–≥–æ —à–∞–≥–æ–≤: {total_steps_normal:.0f}")
    print(f"   –®–∞–≥–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É: {total_steps_normal / time_normal:.2f}")
    print()
    
    # 2. –ù–∞–∏–≤–Ω–∞—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è (SyncVectorEnv)
    print("="*60)
    print("2. –ù–ê–ò–í–ù–ê–Ø –í–ï–ö–¢–û–†–ò–ó–ê–¶–ò–Ø (SyncVectorEnv)")
    print("="*60)
    
    env_sync = make_vectorized_env(
        num_envs=num_envs,
        grid_size=grid_size,
        start_pos=start_pos,
        goal_pos=goal_pos,
        obstacles=obstacles,
        floor_colors=floor_colors,
        seed=42,
        max_steps=max_steps,
    )
    
    agent_sync = DQNAgent(
        obs_dim=1,
        action_dim=4,
        lr=1e-3,
        gamma=0.99,
        epsilon_start=1.0,
        epsilon_end=0.01,
        epsilon_decay=0.995,
        buffer_size=10000,
        batch_size=32,
        device='cuda' if torch.cuda.is_available() else 'cpu',
    )
    
    start_time_sync = time.time()
    
    metrics_sync = train_dqn_vectorized(
        env_sync,
        agent_sync,
        n_episodes=n_episodes,
        max_steps_per_episode=max_steps,
        train_freq=4,
        eval_freq=50,
        save_dir=None,
        use_wandb=use_wandb,
        wandb_config={
            'project': wandb_project,
            'name': f'sync-vectorized-env{env_id}',
            'config': {
                'env_id': env_id,
                'num_envs': num_envs,
                'vectorized': True,
                'vectorization_type': 'sync',
            }
        } if use_wandb else None,
    )
    
    time_sync = time.time() - start_time_sync
    # –î–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–π —Å—Ä–µ–¥—ã total_steps = —Å—É–º–º–∞ –ø–æ –≤—Å–µ–º —ç–ø–∏–∑–æ–¥–∞–º –∏ –≤—Å–µ–º —Å—Ä–µ–¥–∞–º
    # –ö–∞–∂–¥—ã–π —ç–ø–∏–∑–æ–¥ –¥–∞–µ—Ç num_envs —à–∞–≥–æ–≤ (–ø–æ—Ç–æ–º—É —á—Ç–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ)
    if 'total_steps' in metrics_sync and metrics_sync['total_steps']:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ total_steps (–∫–æ—Ç–æ—Ä–æ–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—É—é —Å—É–º–º—É)
        total_steps_sync = metrics_sync['total_steps'][-1] if metrics_sync['total_steps'] else 0
    else:
        # –§–æ–ª–ª–±—ç–∫: —Å—É–º–º–∞ –¥–ª–∏–Ω —ç–ø–∏–∑–æ–¥–æ–≤ * num_envs
        total_steps_sync = sum(metrics_sync['episode_lengths']) * num_envs if metrics_sync['episode_lengths'] else 0
    
    results['sync'] = {
        'metrics': metrics_sync,
        'time': time_sync,
        'total_steps': total_steps_sync,
        'steps_per_second': total_steps_sync / time_sync if time_sync > 0 else 0,
        'final_reward': np.mean(metrics_sync['episode_rewards'][-100:]) if len(metrics_sync['episode_rewards']) >= 100 else 0,
        'num_episodes': len(metrics_sync['episode_rewards']),
        'speedup': time_normal / time_sync if time_sync > 0 else 0,
    }
    
    print(f"‚úÖ SyncVectorEnv –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    print(f"   –í—Ä–µ–º—è: {time_sync:.2f}s")
    print(f"   –í—Å–µ–≥–æ —à–∞–≥–æ–≤: {total_steps_sync:.0f}")
    print(f"   –®–∞–≥–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É: {total_steps_sync / time_sync:.2f}")
    print(f"   –£—Å–∫–æ—Ä–µ–Ω–∏–µ: {time_normal / time_sync:.2f}x")
    print()
    
    # 3. –ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω–∞—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è (numpy)
    print("="*60)
    print("3. –ü–û–õ–ù–û–¶–ï–ù–ù–ê–Ø –í–ï–ö–¢–û–†–ò–ó–ê–¶–ò–Ø (numpy)")
    print("="*60)
    
    env_vectorized = VectorizedGridWorldEnv(
        num_envs=num_envs,
        grid_size=grid_size,
        start_pos=start_pos,
        goal_pos=goal_pos,
        obstacles=obstacles,
        floor_colors=floor_colors,
        seed=42,
        max_steps=max_steps,
    )
    
    agent_vectorized = DQNAgent(
        obs_dim=1,
        action_dim=4,
        lr=1e-3,
        gamma=0.99,
        epsilon_start=1.0,
        epsilon_end=0.01,
        epsilon_decay=0.995,
        buffer_size=10000,
        batch_size=32,
        device='cuda' if torch.cuda.is_available() else 'cpu',
    )
    
    start_time_vectorized = time.time()
    
    metrics_vectorized = train_dqn_vectorized(
        env_vectorized,
        agent_vectorized,
        n_episodes=n_episodes,
        max_steps_per_episode=max_steps,
        train_freq=4,
        eval_freq=50,
        save_dir=None,
        use_wandb=use_wandb,
        wandb_config={
            'project': wandb_project,
            'name': f'full-vectorized-env{env_id}',
            'config': {
                'env_id': env_id,
                'num_envs': num_envs,
                'vectorized': True,
                'vectorization_type': 'full',
            }
        } if use_wandb else None,
    )
    
    time_vectorized = time.time() - start_time_vectorized
    # –î–ª—è –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–π –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º total_steps
    if 'total_steps' in metrics_vectorized and metrics_vectorized['total_steps']:
        total_steps_vectorized = metrics_vectorized['total_steps'][-1] if metrics_vectorized['total_steps'] else 0
    else:
        total_steps_vectorized = sum(metrics_vectorized['episode_lengths']) * num_envs if metrics_vectorized['episode_lengths'] else 0
    
    results['full'] = {
        'metrics': metrics_vectorized,
        'time': time_vectorized,
        'total_steps': total_steps_vectorized,
        'steps_per_second': total_steps_vectorized / time_vectorized if time_vectorized > 0 else 0,
        'final_reward': np.mean(metrics_vectorized['episode_rewards'][-100:]) if len(metrics_vectorized['episode_rewards']) >= 100 else 0,
        'num_episodes': len(metrics_vectorized['episode_rewards']),
        'speedup': time_normal / time_vectorized if time_vectorized > 0 else 0,
    }
    
    print(f"‚úÖ –ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω–∞—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    print(f"   –í—Ä–µ–º—è: {time_vectorized:.2f}s")
    print(f"   –í—Å–µ–≥–æ —à–∞–≥–æ–≤: {total_steps_vectorized:.0f}")
    print(f"   –®–∞–≥–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É: {total_steps_vectorized / time_vectorized:.2f}")
    print(f"   –£—Å–∫–æ—Ä–µ–Ω–∏–µ: {time_normal / time_vectorized:.2f}x")
    print()
    
    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    print("="*80)
    print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –°–†–ê–í–ù–ï–ù–ò–Ø")
    print("="*80)
    print(f"{'–ú–µ—Ç–æ–¥':<25} {'–í—Ä–µ–º—è (—Å)':<12} {'–≠–ø–∏–∑–æ–¥–æ–≤':<12} {'–í—Å–µ–≥–æ —à–∞–≥–æ–≤':<15} {'–®–∞–≥–æ–≤/—Å':<12} {'–£—Å–∫–æ—Ä–µ–Ω–∏–µ':<12} {'–ù–∞–≥—Ä–∞–¥–∞':<10}")
    print("-"*100)
    
    print(f"{'–û–±—ã—á–Ω–∞—è':<25} {time_normal:<12.2f} {results['normal']['num_episodes']:<12} "
          f"{total_steps_normal:<15.0f} {results['normal']['steps_per_second']:<12.2f} "
          f"{'1.00x':<12} {results['normal']['final_reward']:<10.2f}")
    
    print(f"{'SyncVectorEnv':<25} {time_sync:<12.2f} {results['sync']['num_episodes']:<12} "
          f"{total_steps_sync:<15.0f} {results['sync']['steps_per_second']:<12.2f} "
          f"{results['sync']['speedup']:<12.2f}x {results['sync']['final_reward']:<10.2f}")
    
    print(f"{'–ü–æ–ª–Ω–∞—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è':<25} {time_vectorized:<12.2f} {results['full']['num_episodes']:<12} "
          f"{total_steps_vectorized:<15.0f} {results['full']['steps_per_second']:<12.2f} "
          f"{results['full']['speedup']:<12.2f}x {results['full']['final_reward']:<10.2f}")
    print()
    
    print("üìä –í–∞–∂–Ω—ã–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è:")
    print(f"   - SyncVectorEnv: —É—Å–∫–æ—Ä–µ–Ω–∏–µ {results['sync']['speedup']:.2f}x, "
          f"–≤—Å–µ–≥–æ —à–∞–≥–æ–≤: {total_steps_sync:.0f} (–ø—Ä–æ—Ç–∏–≤ {total_steps_normal:.0f} –¥–ª—è –æ–±—ã—á–Ω–æ–π)")
    print(f"   - –ü–æ–ª–Ω–∞—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è: —É—Å–∫–æ—Ä–µ–Ω–∏–µ {results['full']['speedup']:.2f}x, "
          f"–≤—Å–µ–≥–æ —à–∞–≥–æ–≤: {total_steps_vectorized:.0f}")
    print()
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    os.makedirs(save_dir, exist_ok=True)
    import pickle
    with open(os.path.join(save_dir, f'comparison_env{env_id}.pkl'), 'wb') as f:
        pickle.dump(results, f)
    
    return results


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ–±—ã—á–Ω–æ–π –∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ä–µ–¥')
    parser.add_argument('--env', type=int, default=1, help='ID –æ–∫—Ä—É–∂–µ–Ω–∏—è (1, 2, –∏–ª–∏ 3)')
    parser.add_argument('--num-envs', type=int, default=4, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö —Å—Ä–µ–¥')
    parser.add_argument('--episodes', type=int, default=500, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤')
    parser.add_argument('--wandb', action='store_true', help='–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Wandb')
    parser.add_argument('--wandb-project', type=str, default='RL4', help='–ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞ –≤ Wandb')
    
    args = parser.parse_args()
    
    run_comparison(
        env_id=args.env,
        num_envs=args.num_envs,
        n_episodes=args.episodes,
        use_wandb=args.wandb,
        wandb_project=args.wandb_project,
    )

