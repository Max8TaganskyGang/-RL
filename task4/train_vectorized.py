"""
Training pipeline –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ä–µ–¥
–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∫–∞–∫ SyncVectorEnv, —Ç–∞–∫ –∏ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–π –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
"""

import numpy as np
import torch
from typing import Dict, List, Optional
from collections import deque
import time
import os
import sys

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False
    print("Warning: wandb not available. Install with: pip install wandb")

from dqn import DQNAgent
from ppo import PPOAgent


def train_dqn_vectorized(
    env,
    agent: DQNAgent,
    n_episodes: int = 1000,
    max_steps_per_episode: int = 200,
    train_freq: int = 4,
    eval_freq: int = 50,
    eval_episodes: int = 10,
    save_dir: Optional[str] = None,
    use_wandb: bool = False,
    wandb_config: Optional[Dict] = None,
) -> Dict[str, List[float]]:
    """
    –û–±—É—á–µ–Ω–∏–µ DQN –∞–≥–µ–Ω—Ç–∞ –Ω–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–π —Å—Ä–µ–¥–µ.
    
    Args:
        env: –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Å—Ä–µ–¥–∞ (SyncVectorEnv –∏–ª–∏ VectorizedGridWorldEnv)
        agent: DQN –∞–≥–µ–Ω—Ç
        n_episodes: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤ –æ–±—É—á–µ–Ω–∏—è (—Å—É–º–º–∞—Ä–Ω–æ –ø–æ –≤—Å–µ–º —Å—Ä–µ–¥–∞–º)
        max_steps_per_episode: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –≤ —ç–ø–∏–∑–æ–¥–µ
        train_freq: –ß–∞—Å—Ç–æ—Ç–∞ –æ–±—É—á–µ–Ω–∏—è (–∫–∞–∂–¥—ã–µ N —à–∞–≥–æ–≤)
        eval_freq: –ß–∞—Å—Ç–æ—Ç–∞ –æ—Ü–µ–Ω–∫–∏ (–∫–∞–∂–¥—ã–µ N —ç–ø–∏–∑–æ–¥–æ–≤)
        eval_episodes: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
        save_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        use_wandb: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ Wandb –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        wandb_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è Wandb
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è
    """
    metrics = {
        'episode_rewards': [],
        'episode_lengths': [],
        'losses': [],
        'eval_rewards': [],
        'eval_lengths': [],
        'total_steps': [],
    }
    
    episode_rewards = deque(maxlen=100)
    episode_lengths = deque(maxlen=100)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö —Å—Ä–µ–¥
    if hasattr(env, 'num_envs'):
        num_envs = env.num_envs
    elif hasattr(env, 'num_envs'):
        num_envs = env.num_envs
    else:
        num_envs = getattr(env, 'num_envs', 1)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è wandb
    if use_wandb and WANDB_AVAILABLE:
        if wandb_config is None:
            wandb_config = {}
        try:
            wandb.init(
                entity='sinitskii-mi',
                project=wandb_config.get('project', 'gridworld-vectorized'),
                name=wandb_config.get('name', 'dqn-vectorized'),
                config={
                    **wandb_config.get('config', {}),
                    'num_envs': num_envs,
                },
                reinit=True,
                mode='online'
            )
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ Wandb: {e}. –ü—Ä–æ–¥–æ–ª–∂–∞—é –æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ Wandb.")
            use_wandb = False
    
    # –°–±—Ä–æ—Å —Å—Ä–µ–¥—ã
    obs, info = env.reset()
    # obs shape: (num_envs, obs_shape)
    
    total_steps = 0
    episode_count = 0
    step_count = 0
    
    # –ë—É—Ñ–µ—Ä—ã –¥–ª—è –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
    episode_rewards_buf = np.zeros(num_envs)
    episode_lengths_buf = np.zeros(num_envs)
    done_buf = np.zeros(num_envs, dtype=bool)
    
    start_time = time.time()
    
    print(f"üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è DQN –Ω–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–π —Å—Ä–µ–¥–µ ({num_envs} –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö —Å—Ä–µ–¥)")
    print(f"üìä –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤: {n_episodes}")
    print()
    
    while episode_count < n_episodes:
        # –í—ã–±–æ—Ä –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è –≤—Å–µ—Ö —Å—Ä–µ–¥
        if hasattr(agent, 'use_lstm') and agent.use_lstm:
            # –î–ª—è LSTM –Ω—É–∂–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∫–∞–∂–¥—É—é —Å—Ä–µ–¥—É –æ—Ç–¥–µ–ª—å–Ω–æ
            actions = []
            for i in range(num_envs):
                if done_buf[i]:
                    # –°–±—Ä–æ—Å —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö —Å—Ä–µ–¥
                    agent.reset_hidden_state()
                action = agent.select_action(obs[i], training=True, reset_hidden=done_buf[i])
                actions.append(action)
            actions = np.array(actions)
        else:
            # –î–ª—è –æ–±—ã—á–Ω–æ–≥–æ DQN –º–æ–∂–µ–º –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å batch
            actions = []
            for i in range(num_envs):
                if not done_buf[i]:  # –¢–æ–ª—å–∫–æ –¥–ª—è –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å—Ä–µ–¥
                    action = agent.select_action(obs[i], training=True)
                    actions.append(action)
                else:
                    actions.append(0)  # Dummy action –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö —Å—Ä–µ–¥
            actions = np.array(actions)
        
        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π –≤–æ –≤—Å–µ—Ö —Å—Ä–µ–¥–∞—Ö
        next_obs, rewards, terminated, truncated, info = env.step(actions)
        done = terminated | truncated
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –±—É—Ñ–µ—Ä—ã –¥–ª—è –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å—Ä–µ–¥
        active_mask = ~done_buf
        episode_rewards_buf[active_mask] += rewards[active_mask]
        episode_lengths_buf[active_mask] += 1
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö —ç–ø–∏–∑–æ–¥–æ–≤
        newly_done = done & ~done_buf
        if np.any(newly_done):
            for i in np.where(newly_done)[0]:
                episode_rewards.append(float(episode_rewards_buf[i]))
                episode_lengths.append(int(episode_lengths_buf[i]))
                
                metrics['episode_rewards'].append(float(episode_rewards_buf[i]))
                metrics['episode_lengths'].append(int(episode_lengths_buf[i]))
                
                if use_wandb and WANDB_AVAILABLE:
                    wandb.log({
                        'episode_reward': float(episode_rewards_buf[i]),
                        'episode_length': int(episode_lengths_buf[i]),
                        'episode': episode_count,
                        'total_steps': total_steps,
                    })
                
                episode_count += 1
                
                # –°–±—Ä–æ—Å –±—É—Ñ–µ—Ä–æ–≤ –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö —Å—Ä–µ–¥
                episode_rewards_buf[i] = 0
                episode_lengths_buf[i] = 0
                done_buf[i] = False  # –°—Ä–µ–¥–∞ –±—É–¥–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–±—Ä–æ—à–µ–Ω–∞
                
                if episode_count % 100 == 0:
                    avg_reward = np.mean(episode_rewards) if episode_rewards else 0
                    avg_length = np.mean(episode_lengths) if episode_lengths else 0
                    elapsed = time.time() - start_time
                    print(f"Episode {episode_count}/{n_episodes} | "
                          f"Avg Reward: {avg_reward:.2f} | "
                          f"Avg Length: {avg_length:.2f} | "
                          f"Total Steps: {total_steps} | "
                          f"Time: {elapsed:.1f}s")
        
        # –û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞
        step_count += 1
        total_steps += num_envs
        
        if step_count % train_freq == 0 and len(episode_rewards) > 0:
            # –û–±—É—á–∞–µ–º –∞–≥–µ–Ω—Ç–∞ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å—Ä–µ–¥
            # –ó–¥–µ—Å—å –Ω—É–∂–Ω–æ –Ω–∞–∫–∞–ø–ª–∏–≤–∞—Ç—å transitions –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            # –î–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è, –º—ã –±—É–¥–µ–º –æ–±—É—á–∞—Ç—å –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –∏–∑ replay buffer
            # –ù–æ —ç—Ç–æ —Ç—Ä–µ–±—É–µ—Ç –¥–æ—Ä–∞–±–æ—Ç–∫–∏ replay buffer –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å batch
            pass
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ –≤ replay buffer (–¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–∫—Ä—É–∂–µ–Ω–∏—è)
        for i in range(num_envs):
            if not done_buf[i]:
                agent.replay_buffer.push(
                    obs[i],
                    actions[i],
                    rewards[i],
                    next_obs[i],
                    done[i]
                )
                
                # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ batch –∏–∑ replay buffer
                if len(agent.replay_buffer) > agent.batch_size and step_count % train_freq == 0:
                    loss = agent.train_step()
                    if loss is not None:
                        metrics['losses'].append(loss)
                        if use_wandb and WANDB_AVAILABLE:
                            wandb.log({
                                'loss': loss,
                                'total_steps': total_steps,
                            })
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π
        obs = next_obs
        done_buf = done
        
        # –°–±—Ä–æ—Å –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö —Å—Ä–µ–¥
        if np.any(done):
            reset_indices = np.where(done)[0]
            for i in reset_indices:
                # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ —Å—Ä–µ–¥—ã
                # –î–ª—è SyncVectorEnv —ç—Ç–æ –¥–µ–ª–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
                # –î–ª—è VectorizedGridWorldEnv –Ω—É–∂–Ω–æ –≤—ã–∑—ã–≤–∞—Ç—å reset —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω—É–∂–Ω—ã—Ö —Å—Ä–µ–¥
                if hasattr(env, 'reset_single'):
                    obs[i], _ = env.reset_single(i)
                else:
                    # –ï—Å–ª–∏ —Å—Ä–µ–¥–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —á–∞—Å—Ç–∏—á–Ω—ã–π reset, —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –≤—Å–µ
                    obs, info = env.reset()
                    break
        
        # –û—Ü–µ–Ω–∫–∞
        if episode_count > 0 and episode_count % eval_freq == 0:
            eval_reward, eval_length = evaluate_vectorized(env, agent, eval_episodes)
            metrics['eval_rewards'].append(eval_reward)
            metrics['eval_lengths'].append(eval_length)
            
            if use_wandb and WANDB_AVAILABLE:
                wandb.log({
                    'eval_reward': eval_reward,
                    'eval_length': eval_length,
                    'eval_episode': episode_count,
                })
            
            print(f"Evaluation | Avg Reward: {eval_reward:.2f} | Avg Length: {eval_length:.2f}")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            if save_dir:
                os.makedirs(save_dir, exist_ok=True)
                torch.save(agent.q_network.state_dict(), 
                          os.path.join(save_dir, f'dqn_episode_{episode_count}.pt'))
    
    elapsed_time = time.time() - start_time
    
    print(f"\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"üìä –í—Å–µ–≥–æ —ç–ø–∏–∑–æ–¥–æ–≤: {episode_count}")
    print(f"üìä –í—Å–µ–≥–æ —à–∞–≥–æ–≤: {total_steps}")
    print(f"‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {elapsed_time:.2f}s")
    print(f"‚ö° –®–∞–≥–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É: {total_steps / elapsed_time:.2f}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
    if save_dir:
        os.makedirs(save_dir, exist_ok=True)
        import pickle
        with open(os.path.join(save_dir, 'metrics.pkl'), 'wb') as f:
            pickle.dump(metrics, f)
    
    if use_wandb and WANDB_AVAILABLE:
        wandb.finish()
    
    return metrics


def evaluate_vectorized(env, agent, n_episodes: int = 10) -> tuple:
    """–û—Ü–µ–Ω–∫–∞ –∞–≥–µ–Ω—Ç–∞ –Ω–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–π —Å—Ä–µ–¥–µ."""
    episode_rewards = []
    episode_lengths = []
    
    obs, info = env.reset()
    done_buf = np.zeros(env.num_envs, dtype=bool)
    episode_rewards_buf = np.zeros(env.num_envs)
    episode_lengths_buf = np.zeros(env.num_envs)
    
    episode_count = 0
    
    while episode_count < n_episodes:
        # –í—ã–±–æ—Ä –¥–µ–π—Å—Ç–≤–∏–π
        actions = []
        for i in range(env.num_envs):
            if not done_buf[i]:
                if hasattr(agent, 'use_lstm') and agent.use_lstm:
                    action = agent.select_action(obs[i], training=False, reset_hidden=done_buf[i])
                else:
                    action = agent.select_action(obs[i], training=False)
                actions.append(action)
            else:
                actions.append(0)
        actions = np.array(actions)
        
        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π
        next_obs, rewards, terminated, truncated, info = env.step(actions)
        done = terminated | truncated
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±—É—Ñ–µ—Ä–æ–≤
        active_mask = ~done_buf
        episode_rewards_buf[active_mask] += rewards[active_mask]
        episode_lengths_buf[active_mask] += 1
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö —ç–ø–∏–∑–æ–¥–æ–≤
        newly_done = done & ~done_buf
        if np.any(newly_done):
            for i in np.where(newly_done)[0]:
                episode_rewards.append(float(episode_rewards_buf[i]))
                episode_lengths.append(int(episode_lengths_buf[i]))
                episode_rewards_buf[i] = 0
                episode_lengths_buf[i] = 0
                done_buf[i] = False
                episode_count += 1
        
        obs = next_obs
        done_buf = done
        
        # –°–±—Ä–æ—Å –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö —Å—Ä–µ–¥
        if np.any(done):
            obs, info = env.reset()
    
    avg_reward = np.mean(episode_rewards) if episode_rewards else 0
    avg_length = np.mean(episode_lengths) if episode_lengths else 0
    
    return avg_reward, avg_length

